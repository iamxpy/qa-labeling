{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "df=pd.read_csv('spelling_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_split'] = df['question_title'].apply(nltk.word_tokenize)\n",
    "df['body_split'] = df['question_body'].apply(nltk.word_tokenize)\n",
    "df['answer_split'] = df['answer'].apply(nltk.word_tokenize)\n",
    "\n",
    "unique_words_title = {word for sentence in df.title_split.values for word in sentence}\n",
    "unique_words_body = {word for sentence in df.body_split.values for word in sentence}\n",
    "unique_words_answer = {word for sentence in df.answer_split.values for word in sentence}\n",
    "\n",
    "unique_words=unique_words_title|unique_words_body|unique_words_answer\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english') + ['']\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "stemmer_dict = {u: stemmer.stem(u) for u in unique_words}\n",
    "\n",
    "df['title_stemmed'] = df['title_split'].apply(lambda x: [stemmer_dict[y] for y in x if re.sub('[^a-z]+','',y.lower()) not in stopwords])\n",
    "\n",
    "\n",
    "df['body_stemmed'] = df['body_split'].apply(lambda x: [stemmer_dict[y] for y in x if re.sub('[^a-z]+','',y.lower()) not in stopwords])\n",
    "\n",
    "\n",
    "df['answer_stemmed'] = df['answer_split'].apply(lambda x: [stemmer_dict[y] for y in x if re.sub('[^a-z]+','',y.lower()) not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_word_features(df,word):\n",
    "    df[f'title_n_{word}']=df.title_stemmed.apply(lambda x: x.count(word))\n",
    "    df[f'body_n_{word}']=df.body_stemmed.apply(lambda x: x.count(word))\n",
    "    df[f'answer_n_{word}']=df.answer_stemmed.apply(lambda x: x.count(word))\n",
    "    df[f'n_{word}']=df[f'title_n_{word}']+df[f'body_n_{word}']+df[f'answer_n_{word}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['mean','definit','syllabl','pronunci','pronounc','sound','grammar','grammat','noun','pronoun','verb','adject','adverb','preposit','conjunct']\n",
    "\n",
    "for word in words:\n",
    "    gen_word_features(df, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 0,\n",
    "    1/3: 1,\n",
    "    2/3: 2\n",
    "}\n",
    "\n",
    "train_y = df['question_type_spelling'].map(classes)\n",
    "train_id = df['qa_id'].values\n",
    "\n",
    "cols_to_drop = ['qa_id', 'question_title', 'question_body', 'answer', 'category', 'host']\n",
    "\n",
    "cols_to_drop += ['title_split','body_split', 'answer_split', 'title_stemmed', 'body_stemmed', 'answer_stemmed']\n",
    "\n",
    "train_X = df.drop(cols_to_drop+['question_type_spelling'], axis=1)\n",
    "# test_X = test_df.drop(cols_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义F1评价函数\n",
    "def f1_score_vail(pred, data_vail):\n",
    "    labels = data_vail.get_label()\n",
    "    print(\"labels.shape: \",labels.shape)\n",
    "    print(\"pred.shape: \", pred.shape)\n",
    "    score_vail = f1_score(y_true=labels, y_pred=pred, average='macro')      # xgb的predict输出即为对应的label\n",
    "    return '1-f1_score', 1-score_vail   # xgb目标是将目标指标降低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def runXGB(train_X, train_y, valid_X, valid_y,weights=None, seed_val=2019, child=1, colsample=0.3):\n",
    "    \"\"\"\n",
    "    test_X是验证集的特征, test_y是验证集的标签, text_X2是测试集的特征\n",
    "    \"\"\"\n",
    "    param = {}\n",
    "    # param['objective'] = 'multi:softprob'\n",
    "    param['objective'] = 'multi:softmax'\n",
    "    param['eta'] = 0.1  # leanrning rate\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    # param['eval_metric'] = \"mlogloss\"\n",
    "    param['eval_metric'] = \"auc\"\n",
    "    param['min_child_weight'] = child\n",
    "    # param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    if weights is not None:\n",
    "        xgtrain = xgb.DMatrix(train_X, label=train_y, weight=weights)\n",
    "    else:\n",
    "        xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "  \n",
    "    xgvalid = xgb.DMatrix(valid_X, label=valid_y)\n",
    "    # watchlist = [ (xgtrain,'train'), (xgvalid, 'valid') ]\n",
    "    watchlist = [(xgvalid, 'valid') ]\n",
    "    model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50,feval=f1_score_vail, verbose_eval=20)  # 加入weights参数\n",
    "\n",
    "    # model.predict返回的是分布(本任务集对应三分类, 所以shape是[sample_num, 3])\n",
    "    pred_test_y = model.predict(xgvalid, ntree_limit = model.best_ntree_limit)\n",
    " \n",
    "    return pred_test_y, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理类别不平衡问题， 加入样本权重 weights 列. 标签0有303个, 1有7个, 2有4个\n",
    "weight_0 = 4/303\n",
    "weight_1 = 4/7\n",
    "weight_2 = 1.0\n",
    "\n",
    "weight_map={\n",
    "    0: weight_0,\n",
    "    1/3: weight_1,\n",
    "    2/3: weight_2\n",
    "}\n",
    "\n",
    "weights = df['question_type_spelling'].map(weight_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "cv_scores = []\n",
    "# pred_full_test = 0\n",
    "# “3”指的是类别数, 因为要将对训练集的预测结果放到这里，由于每个样本得到一个分布所以shape: [sample_num, 3]\n",
    "pred_train = np.zeros([df.shape[0], 3])  \n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    # 这里的dev其实是训练集, val是验证集\n",
    "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    # pred_val_y是对验证集(即所有训练数据的一部分)的预测结果, pred_test_y是对测试集的预测结果\n",
    "    # w=weights.loc[dev_index]\n",
    "    # pred_val_y, model = runXGB(dev_X, dev_y, val_X, val_y, w)\n",
    "    pred_val_y, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "    # pred_full_test = pred_full_test + pred_test_y\n",
    "    # 把对验证集(即所有训练数据的一部分)的预测结果放入pred_train中, N折都放入后就得到对所有训练数据的预测结果\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(f1_score(val_y, pred_val_y,labels=[0,1,2]))\n",
    "print(\"cv scores : \", cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Create_ensemble(object):\n",
    "    def __init__(self, n_splits, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        no_class = len(np.unique(y))\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n",
    "                                     random_state = random_state).split(X, y))\n",
    "\n",
    "        train_proba = np.zeros((X.shape[0], no_class))\n",
    "        \n",
    "        train_pred = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        f1_scores = np.zeros((len(self.base_models), self.n_splits))\n",
    "        recall_scores = np.zeros((len(self.base_models), self.n_splits))\n",
    " \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            \n",
    "            for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "                \n",
    "                X_train = X[train_idx]\n",
    "                Y_train = y[train_idx]\n",
    "                X_valid = X[valid_idx]\n",
    "                Y_valid = y[valid_idx]\n",
    "                \n",
    "                clf.fit(X_train, Y_train)\n",
    "                \n",
    "                valid_pred = clf.predict(X_valid)\n",
    "                recall  = recall_score(Y_valid, valid_pred, average='macro')\n",
    "                f1 = f1_score(Y_valid, valid_pred, average='macro')\n",
    "                \n",
    "                recall_scores[i][j] = recall\n",
    "                f1_scores[i][j] = f1\n",
    "                \n",
    "                train_pred[valid_idx, i] = valid_pred\n",
    "                \n",
    "                ## Probabilities\n",
    "                valid_proba = clf.predict_proba(X_valid)\n",
    "                train_proba[valid_idx, :] = valid_proba\n",
    "                \n",
    "                print( \"Model- {} and CV- {} recall: {}, f1_score: {}\".format(i, j, recall, f1))\n",
    "            \n",
    "        return train_proba, train_pred"
   ]
  }
 ]
}